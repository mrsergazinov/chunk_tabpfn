Hereâ€™s a clean `README.md` draft that matches your requirements and repo layout:

````markdown
# Chunk-TabPFN

This repository contains an extension to **TabPFN** with chunked attention implementation, benchmarking pipelines on **TabArena**, and supporting scripts/notebooks.

---

## ðŸš€ How to use

We use [uv](https://github.com/astral-sh/uv) for environment management.

```bash
uv sync
````

This installs all dependencies specified in `pyproject.toml`.

Make sure your PYTHONPATH env var is set to the project root:

```bash
export PYTHONPATH=$PYTHONPATH:$(pwd)
```

---

## ðŸ“‚ Main contributions

Our contributions live in [`src/tabpfn/extensions`](src/tabpfn/extensions):

* **For math kernel** implementation â€” pure PyTorch.
* **For efficient attention** implementation â€” memory- and compute-efficient attention.

---

## ðŸ§ª Running experiments on TabArena

We provide scripts to run different scales of experiments:

* `run_single_tabarena_large_task_manual.py` â€” run on a *large* TabArena dataset with a given grid of context lengths, recording all the metrics and performance. 
* `run_single_tabarena_task.py` â€” run on a single TabArena task.
* `tabarena_leaderboard.py` â€” aggregate results into leaderboard format.

Results for large datasets are saved under `results/`.

Results for TabArena are stored in `./tabarena_benchmarking_examples/tabarena_minimal_example/custom_tabpfn`.

---

## ðŸ““ Notebooks

For debugging and experimentation:

* [`demo_notebook.ipynb`](demo_notebook.ipynb) â€” quick demo of pipeline.
* [`notebook_analyze_parity.ipynb`](notebook_analyze_parity.ipynb) â€” parity analysis and long-sequence plotting.

Figures generated by notebooks are stored under `figures/`.

---

## ðŸ“– Citing

This work builds on the following excellent repositories:

* [**TabRepo**](https://github.com/autogluon/tabrepo)
* [**TabArena**](https://github.com/TabArena)
* [**TabPFN**](https://github.com/PriorLabs/TabPFN)
* [**ChunkLlama**](hhttps://github.com/HKUNLP/ChunkLlama)

Please cite them if you use this codebase in your research.